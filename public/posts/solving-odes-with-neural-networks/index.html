<!DOCTYPE html>
<html lang="en"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <style>
        :root {
            --accent-color: {
                    {
                    .Site.Params.AccentColor | default "#FF4D4D"
                }
            }

            ;

            --font-size: {
                    {
                    .Site.Params.FontSize | default "17.5px"
                }
            }

            ;
        }
    </style>

    
    
    
    
    
    

    
    <title>Solving Ordinary Differential Equation using Neural Networks</title>
    <meta name="description" content="Solving Ordinary Differential Equation using Neural Networks Neural networks train based on backpropogated errors signals from the training data. But what if …">
    <meta name="keywords" content='physics, neural-networks'>

    <meta property="og:url" content="http://localhost:1313/posts/solving-odes-with-neural-networks/">
    <meta property="og:type" content="website">
    <meta property="og:title" content="Solving Ordinary Differential Equation using Neural Networks">
    <meta property="og:description" content="Solving Ordinary Differential Equation using Neural Networks Neural networks train based on backpropogated errors signals from the training data. But what if …">
    <meta property="og:image" content="http://localhost:1313/images/profile.webp">
    <meta property="og:image:secure_url" content="http://localhost:1313/images/profile.webp">

    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Solving Ordinary Differential Equation using Neural Networks">
    <meta name="twitter:description" content="Solving Ordinary Differential Equation using Neural Networks Neural networks train based on backpropogated errors signals from the training data. But what if …">
    <meta property="twitter:domain" content="http://localhost:1313/posts/solving-odes-with-neural-networks/">
    <meta property="twitter:url" content="http://localhost:1313/posts/solving-odes-with-neural-networks/">
    <meta name="twitter:image" content="http://localhost:1313/images/profile.webp">

    
    <link rel="canonical" href="http://localhost:1313/posts/solving-odes-with-neural-networks/">

    
    <link rel="stylesheet" type="text/css" href="/css/normalize.min.css" media="print">

    
    <link rel="stylesheet" type="text/css" href="/css/main.min.css">

    
    <link id="dark-theme" rel="stylesheet" href="/css/dark.min.css">

    
    <script src="/js/bundle.min.c856832346537b787d5a768476f58b2d6d8117e9f2f05b92c2a7cce81afe1cb0.js" integrity="sha256-yFaDI0ZTe3h9WnaEdvWLLW2BF&#43;ny8FuSwqfM6Br&#43;HLA="></script>

    
    

    
    
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
    MathJax = {
        tex: {
            displayMath: [['\\[', '\\]']],  
            inlineMath: [['\\(', '\\)']]                  
        }
    };
</script>
    
</head><body>
        <script>
            
            setThemeByUserPref();
        </script><header class="header">
    <nav class="header-nav">

        <div class="nav-title">
            <a class="nav-brand" href="http://localhost:1313/">Jack Montgomery</a>
        </div>

        <div class="nav-links">

            <div class="dark-theme-toggle">
                <span class="sr-only dark-theme-toggle-screen-reader-target">theme</span>
                <a aria-hidden="true" role="switch">
                    <span class="theme-toggle-icon" data-feather="moon"></span>
                </a>
            </div>

            <span class="nav-icons-divider"></span>

            
            <div class="nav-link">
                <a href="http://localhost:1313/" aria-label="" ><span data-feather='home'></span> Home </a>
            </div>
            
            <div class="nav-link">
                <a href="http://localhost:1313/posts/" aria-label="" ><span data-feather='book'></span> Posts </a>
            </div>
            
            <div class="nav-link">
                <a href="http://localhost:1313/tags/" aria-label="" ><span data-feather='tag'></span> Tags </a>
            </div>
            
            <div class="nav-link">
                <a href="http://localhost:1313/pdfs/jack_montgomery_cv.pdf" aria-label="" target="_blank"><span data-feather='file-text' ></span> CV </a>
            </div>
            

            <div class="nav-link" id="hamburger-menu-toggle">
                <span class="sr-only hamburger-menu-toggle-screen-reader-target">menu</span>
                <a aria-checked="false" aria-labelledby="hamburger-menu-toggle" id="hamburger-menu-toggle-target"
                    role="switch">
                    <span data-feather="menu"></span>
                </a>
            </div>

            
            <ul class="nav-hamburger-list visibility-hidden">
                
                <li class="nav-item">
                    <a href="http://localhost:1313/" ><span data-feather='home'></span> Home </a>
                </li>
                
                <li class="nav-item">
                    <a href="http://localhost:1313/posts/" ><span data-feather='book'></span> Posts </a>
                </li>
                
                <li class="nav-item">
                    <a href="http://localhost:1313/tags/" ><span data-feather='tag'></span> Tags </a>
                </li>
                
                <li class="nav-item">
                    <a href="http://localhost:1313/pdfs/jack_montgomery_cv.pdf" target="_blank"><span data-feather='file-text' ></span> CV </a>
                </li>
                
            </ul>
        </div>
    </nav>
</header><main id="content">
    <div class="post container">
    <div class="post-header-section">

        

        
        
        
        
        
        

        <p></p>

        

        

        
        <small role="doc-subtitle"></small>
        

        
        <p class="post-date">January 22, 2025
             | Updated January 24, 2025
        </p>
        

        <ul class="post-tags">
            
            
            <li class="post-tag"><a href="http://localhost:1313/%20tags/physics">physics</a></li>
            
            
            
            <li class="post-tag"><a href="http://localhost:1313/%20tags/neural-networks">neural-networks</a></li>
            
            
        </ul>
    </div>

    <div class="post-content">
        <h1 id="solving-ordinary-differential-equation-using-neural-networks">Solving Ordinary Differential Equation using Neural Networks</h1>
<p>Neural networks train based on backpropogated errors signals from the training data. But what if there were some additional constraint on the results of the network that could help the training. When we consider <strong>physical</strong> constraints on the system in order to facilitate learning in the neural network we obtain what is called <strong>physics informed neural networks (PINN)</strong>.</p>
<p>The particular use case we will consider for PINN&rsquo;s is solving ordinary differential equations. This post is inspired by the the work of <a href="https://arxiv.org/abs/2302.12260">Hubert Baty, Leo Baty</a>.</p>
<p>First Equation to Solve:</p>
\[
\frac{dy}{dt} + 0.1y - \sin (\pi * t/2) = 0 
\]<p>Solving a differential equation means finding \(y(t)\) such that this equation is satisfied. But, solving differential equations is hard and is mostly impossible. So, we often turn to numerical schemes in order to approximate the solution.</p>
<h2 id="exact-solution-using-numerical-integration">Exact Solution using numerical integration</h2>
<p>We will use the Runge-Kutta 4 (RK4) as our numerical integrator to plot the trajectory of the solution in \(t \in [0, 30]\). The RK4 method uses the expressed stated as a function of the first derivative, that is:</p>
\[
\frac{dy}{dt} = \sin (\pi * t/2) - 0.1y
\]<p>With the initial condition: \(y(0) = 1\).</p>
<p>Using this, the method then estimates the change in the value of y for a given step size. Let us see this in action:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>style<span style="color:#f92672">.</span>use(<span style="color:#e6db74">&#34;ggplot&#34;</span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">rk4</span>(f, t, y, h):
</span></span><span style="display:flex;"><span>    k1 <span style="color:#f92672">=</span> h <span style="color:#f92672">*</span> f(t, y)
</span></span><span style="display:flex;"><span>    k2 <span style="color:#f92672">=</span> h <span style="color:#f92672">*</span> f(t <span style="color:#f92672">+</span> h <span style="color:#f92672">/</span> <span style="color:#ae81ff">2</span>, y <span style="color:#f92672">+</span> k1 <span style="color:#f92672">/</span> <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>    k3 <span style="color:#f92672">=</span> h <span style="color:#f92672">*</span> f(t <span style="color:#f92672">+</span> h <span style="color:#f92672">/</span> <span style="color:#ae81ff">2</span>, y <span style="color:#f92672">+</span> k2 <span style="color:#f92672">/</span> <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>    k4 <span style="color:#f92672">=</span> h <span style="color:#f92672">*</span> f(t <span style="color:#f92672">+</span> h, y <span style="color:#f92672">+</span> k3)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> y <span style="color:#f92672">+</span> (k1 <span style="color:#f92672">+</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> k2 <span style="color:#f92672">+</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> k3 <span style="color:#f92672">+</span> k4) <span style="color:#f92672">/</span> <span style="color:#ae81ff">6</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">solve_ode</span>(f, t0, y0, t_end, h):
</span></span><span style="display:flex;"><span>    t_values <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(t0, t_end <span style="color:#f92672">+</span> h, h)
</span></span><span style="display:flex;"><span>    y_values <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((len(t_values),) <span style="color:#f92672">+</span> np<span style="color:#f92672">.</span>shape(y0))
</span></span><span style="display:flex;"><span>    y_values[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">=</span> y0
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">1</span>, len(t_values)):
</span></span><span style="display:flex;"><span>        y_values[i] <span style="color:#f92672">=</span> rk4(f, t_values[i <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>], y_values[i <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>], h)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> t_values, y_values
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">dy_dt</span>(t, y):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>sin(np<span style="color:#f92672">.</span>pi <span style="color:#f92672">*</span> t <span style="color:#f92672">/</span> <span style="color:#ae81ff">2</span> ) <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span><span style="color:#f92672">/</span><span style="color:#ae81ff">10</span> <span style="color:#f92672">*</span> y
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>t_values, y_values <span style="color:#f92672">=</span> solve_ode(dy_dt, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">30</span>, <span style="color:#ae81ff">0.01</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#34;t&#34;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#34;y(t)&#34;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot(t_values, y_values)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><p><img src="output_3_0.png" alt="png"></p>
<p>This is the exact solution to the differential equation. Well not exact since we have used a numerical integrator, but with a simple, non-chaotic system here integrated over a small time scale we can be pretty confident.</p>
<p>But is to note is that the only way we can find the solution using this method is to step through the integrator. There is no machine where we can plug in a time and it will spit out the y - like what a function does - we can only tell where the particle will be at time t by starting at time 0 and walking through.</p>
<h2 id="use-a-neural-network-with-no-physical-information">Use a neural network with no &ldquo;physical&rdquo; information</h2>
<p>We can use neural networks as a universal approximator in order to try and learn the solution for \(y(t)\). We can do this by taking a sample of the solution data set that we obtained using numerical integration, and iteratively learning based on these samples. Let us first create a basic neural network using PyTorch as well as a result plotting function.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn <span style="color:#66d9ef">as</span> nn
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.optim <span style="color:#66d9ef">as</span> optim
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">NN</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, n_input<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, n_output<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, hidden_dim<span style="color:#f92672">=</span><span style="color:#ae81ff">32</span>):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>activation <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Tanh()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>fc1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(n_input, hidden_dim)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>fc2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(hidden_dim, hidden_dim)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>fc3 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(hidden_dim, n_output)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>fc1(x)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>activation(x)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>fc2(x)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>activation(x)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>fc3(x)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">plot_result</span>(x, y, x_data, y_data, yh, xp<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;Pretty plot training results&#34;</span>
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">4</span>))
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>plot(x, yh, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;tab:red&#34;</span>, linewidth<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.8</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;NN prediction&#34;</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>plot(
</span></span><span style="display:flex;"><span>        x,
</span></span><span style="display:flex;"><span>        y,
</span></span><span style="display:flex;"><span>        color<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;blue&#34;</span>,
</span></span><span style="display:flex;"><span>        linewidth<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>,
</span></span><span style="display:flex;"><span>        alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.8</span>,
</span></span><span style="display:flex;"><span>        linestyle<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;--&#34;</span>,
</span></span><span style="display:flex;"><span>        label<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Exact solution&#34;</span>,
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>scatter(x_data, y_data, s<span style="color:#f92672">=</span><span style="color:#ae81ff">60</span>, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;tab:red&#34;</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.4</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Training data&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> xp <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>        plt<span style="color:#f92672">.</span>scatter(
</span></span><span style="display:flex;"><span>            xp,
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">-</span><span style="color:#ae81ff">0</span> <span style="color:#f92672">*</span> torch<span style="color:#f92672">.</span>ones_like(xp),
</span></span><span style="display:flex;"><span>            s<span style="color:#f92672">=</span><span style="color:#ae81ff">30</span>,
</span></span><span style="display:flex;"><span>            color<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;tab:green&#34;</span>,
</span></span><span style="display:flex;"><span>            alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.4</span>,
</span></span><span style="display:flex;"><span>            label<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Collocation points&#34;</span>,
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>    leg <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>legend(loc<span style="color:#f92672">=</span>(<span style="color:#ae81ff">0.67</span>, <span style="color:#ae81ff">0.62</span>), frameon<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>, fontsize<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;large&#34;</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>setp(leg<span style="color:#f92672">.</span>get_texts(), color<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;k&#34;</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>xlim(<span style="color:#f92672">-</span><span style="color:#ae81ff">1.25</span>, <span style="color:#ae81ff">31.05</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>ylim(<span style="color:#f92672">-</span><span style="color:#ae81ff">0.65</span>, <span style="color:#ae81ff">2.25</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>text(<span style="color:#ae81ff">2.965</span>, <span style="color:#ae81ff">1.95</span>, <span style="color:#e6db74">&#34;Training step: </span><span style="color:#e6db74">%i</span><span style="color:#e6db74">&#34;</span> <span style="color:#f92672">%</span> (i <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>), fontsize<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;xx-large&#34;</span>, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;k&#34;</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#34;y&#34;</span>, fontsize<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;xx-large&#34;</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#34;t&#34;</span>, fontsize<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;xx-large&#34;</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>axis(<span style="color:#e6db74">&#34;on&#34;</span>)
</span></span></code></pre></div><p>We can now sample our training set.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>t_values <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>Tensor(t_values)
</span></span><span style="display:flex;"><span>y_values <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>Tensor(y_values)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>t_train <span style="color:#f92672">=</span> t_values[::<span style="color:#ae81ff">100</span>]
</span></span><span style="display:flex;"><span>y_train <span style="color:#f92672">=</span> y_values[::<span style="color:#ae81ff">100</span>]
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>torch<span style="color:#f92672">.</span>manual_seed(<span style="color:#ae81ff">123</span>)
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> NN()
</span></span><span style="display:flex;"><span>optimiser <span style="color:#f92672">=</span> optim<span style="color:#f92672">.</span>Adam(model<span style="color:#f92672">.</span>parameters(), lr<span style="color:#f92672">=</span><span style="color:#ae81ff">5e-3</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>loss_history <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">25000</span>):
</span></span><span style="display:flex;"><span>    yh <span style="color:#f92672">=</span> model(t_train<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">1</span>))<span style="color:#f92672">.</span>squeeze()
</span></span><span style="display:flex;"><span>    loss <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>mean((yh <span style="color:#f92672">-</span> y_train) <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    optimiser<span style="color:#f92672">.</span>zero_grad()
</span></span><span style="display:flex;"><span>    loss<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>    optimiser<span style="color:#f92672">.</span>step()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> (i <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">%</span> <span style="color:#ae81ff">100</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>        loss_history<span style="color:#f92672">.</span>append(loss<span style="color:#f92672">.</span>detach())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>yh <span style="color:#f92672">=</span> model(t_values<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">1</span>))<span style="color:#f92672">.</span>detach()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Create a single figure with two subplots side by side</span>
</span></span><span style="display:flex;"><span>fig, axs <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">12</span>, <span style="color:#ae81ff">5</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Plot the model results on the first subplot</span>
</span></span><span style="display:flex;"><span>axs[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>plot(t_values, y_values, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;True&#34;</span>)
</span></span><span style="display:flex;"><span>axs[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>scatter(t_train, y_train, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;red&#34;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Train Data&#34;</span>)
</span></span><span style="display:flex;"><span>axs[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>plot(t_values, yh, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Prediction&#34;</span>, linestyle<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;--&#34;</span>)
</span></span><span style="display:flex;"><span>axs[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#34;Model Fit&#34;</span>)
</span></span><span style="display:flex;"><span>axs[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>legend()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Plot the loss history on the second subplot</span>
</span></span><span style="display:flex;"><span>axs[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>plot(loss_history)
</span></span><span style="display:flex;"><span>axs[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>set_xlabel(<span style="color:#e6db74">&#34;Training step (\(10^2\))&#34;</span>, fontsize<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;x-large&#34;</span>)
</span></span><span style="display:flex;"><span>axs[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>set_ylabel(<span style="color:#e6db74">&#34;Loss&#34;</span>, fontsize<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;x-large&#34;</span>)
</span></span><span style="display:flex;"><span>axs[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>set_yscale(<span style="color:#e6db74">&#34;log&#34;</span>)
</span></span><span style="display:flex;"><span>axs[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#34;Training Loss&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>tight_layout()
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><p><img src="output_9_0.png" alt="png"></p>
<p>As we can see the neural networks settles into a good approximation of the solution of the differential equation just before \(200 \times 10^2\) iterations. Naturally, one can conduct some hyperparameter tuning of the neural network, specifically we will look at different learning rates and their effect on the stability of the solution.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>torch<span style="color:#f92672">.</span>manual_seed(<span style="color:#ae81ff">123</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>learning_rates <span style="color:#f92672">=</span> [<span style="color:#ae81ff">5e-2</span>, <span style="color:#ae81ff">5e-3</span>, <span style="color:#ae81ff">1e-3</span>, <span style="color:#ae81ff">5e-4</span>]
</span></span><span style="display:flex;"><span>rows, cols <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span>, len(learning_rates) <span style="color:#f92672">//</span> <span style="color:#ae81ff">2</span>  <span style="color:#75715e"># Define the grid layout</span>
</span></span><span style="display:flex;"><span>fig, axs <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(rows, cols, figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">12</span>, <span style="color:#ae81ff">6</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>all_loss_histories <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> lr <span style="color:#f92672">in</span> learning_rates:
</span></span><span style="display:flex;"><span>    model <span style="color:#f92672">=</span> NN()
</span></span><span style="display:flex;"><span>    optimiser <span style="color:#f92672">=</span> optim<span style="color:#f92672">.</span>Adam(model<span style="color:#f92672">.</span>parameters(), lr<span style="color:#f92672">=</span>lr)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    loss_history <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> j <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">15000</span>):
</span></span><span style="display:flex;"><span>        yh <span style="color:#f92672">=</span> model(t_train<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">1</span>))<span style="color:#f92672">.</span>squeeze()
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>mean((yh <span style="color:#f92672">-</span> y_train) <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        optimiser<span style="color:#f92672">.</span>zero_grad()
</span></span><span style="display:flex;"><span>        loss<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>        optimiser<span style="color:#f92672">.</span>step()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> (j <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">%</span> <span style="color:#ae81ff">100</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>            loss_history<span style="color:#f92672">.</span>append(loss<span style="color:#f92672">.</span>detach())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    all_loss_histories<span style="color:#f92672">.</span>append(loss_history)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Find global min and max for standardizing y-axis</span>
</span></span><span style="display:flex;"><span>min_loss <span style="color:#f92672">=</span> min(min(loss) <span style="color:#66d9ef">for</span> loss <span style="color:#f92672">in</span> all_loss_histories)
</span></span><span style="display:flex;"><span>max_loss <span style="color:#f92672">=</span> max(max(loss) <span style="color:#66d9ef">for</span> loss <span style="color:#f92672">in</span> all_loss_histories)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> idx, (lr, loss_history) <span style="color:#f92672">in</span> enumerate(zip(learning_rates, all_loss_histories)):
</span></span><span style="display:flex;"><span>    row, col <span style="color:#f92672">=</span> divmod(idx, cols)  <span style="color:#75715e"># Calculate row and column indices</span>
</span></span><span style="display:flex;"><span>    axs[row, col]<span style="color:#f92672">.</span>plot(loss_history)
</span></span><span style="display:flex;"><span>    axs[row, col]<span style="color:#f92672">.</span>set_xlabel(<span style="color:#e6db74">&#34;Training step (\(10^2\))&#34;</span>, fontsize<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;x-large&#34;</span>)
</span></span><span style="display:flex;"><span>    axs[row, col]<span style="color:#f92672">.</span>set_ylabel(<span style="color:#e6db74">&#34;Loss&#34;</span>, fontsize<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;x-large&#34;</span>)
</span></span><span style="display:flex;"><span>    axs[row, col]<span style="color:#f92672">.</span>set_yscale(<span style="color:#e6db74">&#34;log&#34;</span>)
</span></span><span style="display:flex;"><span>    axs[row, col]<span style="color:#f92672">.</span>set_ylim(min_loss, max_loss)  <span style="color:#75715e"># Standardize y-axis</span>
</span></span><span style="display:flex;"><span>    axs[row, col]<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;LR: </span><span style="color:#e6db74">{</span>lr<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>tight_layout()
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><p><img src="output_11_0.png" alt="png"></p>
<h2 id="using-a-physical-loss-on-the-neural-network">Using a physical loss on the neural network</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>t_physics <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>linspace(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">30</span>, <span style="color:#ae81ff">50</span>)<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>requires_grad_(<span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>lam <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.1</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>torch<span style="color:#f92672">.</span>manual_seed(<span style="color:#ae81ff">123</span>)
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> NN()
</span></span><span style="display:flex;"><span>optimiser <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>optim<span style="color:#f92672">.</span>Adam(model<span style="color:#f92672">.</span>parameters(), lr<span style="color:#f92672">=</span><span style="color:#ae81ff">3e-2</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>loss_history <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>loss2_history <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">25000</span>):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    optimiser<span style="color:#f92672">.</span>zero_grad()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    yh <span style="color:#f92672">=</span> model(t_train<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">1</span>))<span style="color:#f92672">.</span>squeeze()
</span></span><span style="display:flex;"><span>    loss1 <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.</span><span style="color:#f92672">*</span>torch<span style="color:#f92672">.</span>mean((yh<span style="color:#f92672">-</span>y_train)<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    yhp <span style="color:#f92672">=</span> model(t_physics<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">1</span>))<span style="color:#f92672">.</span>squeeze()
</span></span><span style="display:flex;"><span>    dx <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>autograd<span style="color:#f92672">.</span>grad(yhp, t_physics, torch<span style="color:#f92672">.</span>ones_like(yhp), create_graph<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    physics <span style="color:#f92672">=</span> dx <span style="color:#f92672">+</span> lam <span style="color:#f92672">*</span> yhp <span style="color:#f92672">-</span> torch<span style="color:#f92672">.</span>sin(np<span style="color:#f92672">.</span>pi <span style="color:#f92672">*</span> t_physics <span style="color:#f92672">/</span> <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>    loss2 <span style="color:#f92672">=</span> (<span style="color:#ae81ff">6e-2</span>) <span style="color:#f92672">*</span> (torch<span style="color:#f92672">.</span>mean(physics<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    loss <span style="color:#f92672">=</span> loss1 <span style="color:#f92672">+</span> loss2
</span></span><span style="display:flex;"><span>    loss<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>    optimiser<span style="color:#f92672">.</span>step()
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># We plot the result as training progresses ....................................</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> (i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>) <span style="color:#f92672">%</span> <span style="color:#ae81ff">100</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>        loss_history<span style="color:#f92672">.</span>append(loss<span style="color:#f92672">.</span>detach())
</span></span><span style="display:flex;"><span>        loss2_history<span style="color:#f92672">.</span>append(loss2<span style="color:#f92672">.</span>detach())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>                
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> (i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>) <span style="color:#f92672">%</span> <span style="color:#ae81ff">5000</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>: 
</span></span><span style="display:flex;"><span>            yh <span style="color:#f92672">=</span> model(t_values<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">1</span>))<span style="color:#f92672">.</span>detach()<span style="color:#f92672">.</span>squeeze()
</span></span><span style="display:flex;"><span>            xp <span style="color:#f92672">=</span> t_physics<span style="color:#f92672">.</span>detach()        
</span></span><span style="display:flex;"><span>            plot_result(t_values, y_values, t_train, y_train, yh, xp)
</span></span><span style="display:flex;"><span>            plt<span style="color:#f92672">.</span>show()
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>: 
</span></span><span style="display:flex;"><span>            plt<span style="color:#f92672">.</span>close(<span style="color:#e6db74">&#34;all&#34;</span>)
</span></span><span style="display:flex;"><span>            
</span></span></code></pre></div><p><img src="output_13_0.png" alt="png"></p>
<p><img src="output_13_1.png" alt="png"></p>
<p><img src="output_13_2.png" alt="png"></p>
<p><img src="output_13_3.png" alt="png"></p>
<p><img src="output_13_4.png" alt="png"></p>

        
    </div>

    <div class="prev-next">
        
    </div>

    
    
    
</div>
<aside class="post-toc">
    <nav id="toc">
        <nav id="TableOfContents">
  <ul>
    <li><a href="#exact-solution-using-numerical-integration">Exact Solution using numerical integration</a></li>
    <li><a href="#use-a-neural-network-with-no-physical-information">Use a neural network with no &ldquo;physical&rdquo; information</a></li>
    <li><a href="#using-a-physical-loss-on-the-neural-network">Using a physical loss on the neural network</a></li>
  </ul>
</nav>
    </nav>
</aside>



    

        </main><footer class="footer">
    
    

    

    

    
</footer></body>
</html>
